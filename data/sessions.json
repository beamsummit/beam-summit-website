{
    "1": {
        "icon": "registration",
        "title": "Registration & morning coffee",
        "image": "https://firebasestorage.googleapis.com/v0/b/hoverboard-experimental.appspot.com/o/images%2Fbackgrounds%2Fregistration.jpg?alt=media&token=27328646-d323-4cca-904c-75f021bc3ffe",
        "description": "Get your badge, coffee, enjoy talking to each other"
    },
    "2": {
        "description": "Foooooooood!",
        "icon": "lunch",
        "title": "Lunch",
        "image": "https://firebasestorage.googleapis.com/v0/b/hoverboard-experimental.appspot.com/o/images%2Fbackgrounds%2Flunch.jpg?alt=media&token=bc82aaff-92cc-4dcc-a00e-2b6f30a40225"
    },
    "3": {
        "description": "Coffeeeeeee!",
        "icon": "coffee-break",
        "title": "Coffee break",
        "image": "https://firebasestorage.googleapis.com/v0/b/hoverboard-experimental.appspot.com/o/images%2Fbackgrounds%2Fcoffee-break.jpg?alt=media&token=7c2c929b-cb94-4be8-a05a-b3f678136cb7"
    },
    "4": {
        "description": "Coffeeeeeee!",
        "icon": "coffee-break",
        "title": "Morning coffee",
        "image": "https://firebasestorage.googleapis.com/v0/b/hoverboard-experimental.appspot.com/o/images%2Fbackgrounds%2Fmorning.jpg?alt=media&token=d0da3d64-fc63-48c6-9d41-1460451dca64"
    },
    "5": {
        "description": "Official start of the conference. Greetings from the organizers, sponsors and partners.",
        "icon": "opening",
        "title": "Beam Summit Europe 2019 Opening Ceremony",
        "image": "https://firebasestorage.googleapis.com/v0/b/hoverboard-experimental.appspot.com/o/images%2Fbackgrounds%2Fopening.jpg?alt=media&token=aa52dd45-50c9-4e36-9485-c1f9138ecd6b",
        "language": "English"
    },
    "6": {
        "description": "Afterparty is a great time to have some fun and meet new people",
        "icon": "party",
        "title": "Afterparty & Networking",
        "image": "https://firebasestorage.googleapis.com/v0/b/hoverboard-experimental.appspot.com/o/images%2Fbackgrounds%2Fparty.jpg?alt=media&token=8e413f01-373a-40bf-bb87-1dcf1d8e6af1"
    },
    "7": {
        "description": "Make sure you won't miss closing ceremony!",
        "title": "Conference closing.",
        "image": "https://firebasestorage.googleapis.com/v0/b/hoverboard-experimental.appspot.com/o/images%2Fbackgrounds%2Fclosing.jpg?alt=media&token=f02d014a-09ee-407e-88c7-b74093e00fed"
    },
    "8": {
        "title": "Keynote",
        "language": "English",
        "description": ""
    },
    "9": {
        "title": "TBD",
        "language": "English",
        "description": "TBD"
    },
    "100": {
        "presentation": "https://speakerdeck.com/gdglviv/mete-atamel-windows-and-net-on-google-cloud-platform",
        "complexity": "Beginner",
        "tags": [
            "Use case talk"
        ],
        "speakers": [
            "matthias_baetens"
        ],
        "title": "",
        "language": "English",
        "description": ""
    },
    "101": {
        "videoId": "fLfQNp75kbc",
        "language": "English",
        "description": "",
        "presentation": "https://speakerdeck.com/gdglviv/sascha-wolter-the-physical-web-context-is-king",
        "complexity": "Beginner",
        "tags": [
            "Technical talk"
        ],
        "speakers": [
            "matthias_baetens"
        ],
        "title": ""
    },
    "1000": {
        "description": "In this session we will illustrate how Sky moved from an on-premise, batch-focused Hadoop based analytical system, to a streaming-oriented analytics pipeline deployed using Apache Beam and Google Cloud Dataflow. We will go through the architecture of the system, including the design decision made along the way, as well as learnings from the implementation and deployment process.  We will end the session with an end-to-end demo and code examples!",
        "tags": [],
        "complexity": ["Introductory and overview" ],
        "speakers": [ "matthias_baetens" ],
        "title": "Large scale streaming analytics with Apache Beam"
    },
    "1001": {
        "description": null,
        "tags": [],
        "complexity": [],
        "speakers": [],
        "title": "Keynote"
    },
    "1002": {
        "speakers": [],
        "title": "Coffee Break",
        "description": null,
        "tags": [],
        "complexity": []
    },
    "1003": {
        "description": null,
        "tags": [],
        "complexity": [],
        "speakers": [],
        "title": "Coffee Break"
    },
    "1004": {
        "title": "Workshop Morning",
        "description": "The workshop will be a hands-on overview to Beam.  We will start with a lecture on Apache Beam and Beam model. Hands on examples and exercises will be provided to build up understanding of the basic Beam operations in different languages, and later working our way up to running on various runners, and more complex and advanced use-cases.  At the end of the day attendees with understand the vision for portability and the Beam model and have gained experienced writing and running Beam pipelines.<br><br> 11:00 - 12:00    Presentation on Beam SDK Tutorial. <br> 12:00 - 12:30    Loading and start working on the Beam Katas.",
        "speakers": ["pablo_estrada"]
    },
    "1005": {
        "description": null,
        "tags": [],
        "complexity": [],
        "speakers": [],
        "title": "Lunch"
    },
    "1006": {
        "description": null,
        "tags": [],
        "complexity": [],
        "speakers": [],
        "title": "Lunch"
    },
    "1007": {
        "title": "Workshop Afternoon",
        "description": "The workshop will be a hands-on overview to Beam.  We will start with a lecture on Apache Beam and Beam model. Hands on examples and exercises will be provided to build up understanding of the basic Beam operations in different languages, and later working our way up to running on various runners, and more complex and advanced use-cases.  At the end of the day attendees with understand the vision for portability and the Beam model and have gained experienced writing and running Beam pipelines. <br><br> 12:30 - 13:30    Lunch (and work on the Katas with instructors). <br> 13:30 - 14:30    Further work on Katas. <br> 14:30 - 15:30    Presentation on advanced Beam concepts (SDF/State/Timers). <br> 15:30 - 17:00    Submitting Beam jobs to Flink clusters / Google Cloud Dataflow.",
        "speakers": ["pablo_estrada"]
    },
    "1008": {
        "description": null,
        "tags": [],
        "complexity": [],
        "speakers": [],
        "title": "Coffee Break"
    },
    "1009": {
        "description": null,
        "tags": [],
        "complexity": [],
        "speakers": [],
        "title": "Coffee Break"
    },
    "1010": {
        "speakers": [],
        "title": "Coffee Break",
        "description": null,
        "tags": [],
        "complexity": []
    },
    "1011": {
        "description": null,
        "tags": [],
        "complexity": [],
        "speakers": [],
        "title": "Lunch"
    },
    "1012": {
        "speakers": [],
        "title": "Lunch",
        "description": null,
        "tags": [],
        "complexity": []
    },
    "1013": {
        "speakers": [],
        "title": "Coffee Break",
        "description": null,
        "tags": [],
        "complexity": []
    },
    "1014": {
        "speakers": [],
        "title": "TBA",
        "description": null,
        "tags": [],
        "complexity": []
    },
    "1111": {
        "title": "Apache Arrow + Apache Beam: A vision for cross-language, columnar data pipelines",
        "description": "Apache Arrow and Apache Beam deserve to be together. It is a stated goal of both projects to provide mechanisms for performing data processing tasks across many languages. To accomplish this, both projects define a common binary 'model' and provide multiple language implementations of that model. <br><br>However, they are approaching the overarching problem of cross-language data processing from two very different angles: Arrow is primarily concerned with moving columnar data across language boundaries with minimal overhead and providing optimized computation primitives to operate on that data, while Beam seeks to enable users to write scalable data pipelines in their language of choice and execute them on any distributed data-processing system. There is significant overlap between these two problem spaces, and we should consider using these models together for the benefit of both projects.<br><br>In this talk, I will describe both of these projects at a high-level, and present a vision for how they can be used together. I will also discuss some of the concerns around using a batched columnar data format in the Beam model, along with potential solutions.",
        "speakers": ["brian_hulette"]
    },
    "1112": {
        "description": "Time series processing requires ordered processing of data points, often across windows boundaries. Tricky stuff for distributed processing systems, in this chat we look at how to solve for these problems using a combination of Global Windows, State and the Timer API' with Apache Beam.",
        "speakers": ["reza_ardeshir_rokni"],
        "title": "Using the Timer and State API to solve times series use case in Apache Beam"
    },
    "1113": {
        "description": "Kotlin is an Open Sourced, statically typed language for JVM and is mostly being favoured by Android Developers due to the many myriad features which enable more concise and cleaner code than Java without sacrificing performance or safety.<br><br>By using Kotlin as our language of choice to build an apache beam pipeline, our development team not only ran into fewer errors than what we would have if we used Java; but with the help of extensively useful high level APIs present in Kotlin, we were also able to cut down on the development time required to build, test and deploy a new feature.<br><br>This talk uses real life examples from my personal experiences to advocate on the benefits that you can get by using Kotlin to build solutions in Apache Beam.<br><br>So if you are someone who is already working with Apache Beam's Java SDK, or looking forward to try Apache BEAM, attending this talk will motivate you to go ahead and explore the benefits associated with using Beam with Kotlin.",
        "speakers": ["harshit_dwivedi"],
        "title": "Apache Beam + Kotlin = ❤️"
    },
    "1114": {
        "description": "This session will walk through the internals of what the Go SDK does to execute a pipeline. Describing the structure of the SDK, where code is located, and the why of the structure, and how it does it with the particular features of the Go language, which notably has strict types but no generics.<br><br>Particular effort will be spent on what the Go SDK does for ease of use, and to reduce DoFn execution overhead.",
        "speakers": ["robert_burke"],
        "title": "A Guided Walkthrough of the Apache Beam Go SDK"
    },
    "1115": {
        "description": "Apache Beam is a unified data processing framework, allowing you to write batch and streaming pipelines that run anywhere, including Apache Flink, Apache Spark, and Google Cloud Dataflow. With the SQL extension you can now write a pipeline in pure SQL. If you need more, you can write user defined functions in Java or even embed SQL into your existing Java pipeline.<br><br>This talk will start with a demo pipeline written in pure SQL. We will review how streaming SQL came from collaboration between the Apache Beam, Apache Calcite and Apache Flink communities. Finally, we will deep-dive into the architecture of Beam’s implementation and the work we are doing to make Apache Beam SQL the default choice for writing new streaming pipelines.",
        "speakers": ["andrew_pilloud"],
        "title": "Simple, Portable data pipelines with Apache Beam SQL"
    },
    "1116": {
        "description": "Apache Spark is the most popular open source analytics engine for large-scale data processing. Spark is not only a mature system, but thanks to its support of multiple resource managers like Hadoop, Mesos, and Kubernetes it has become a popular choice for both batch and streaming workloads in the industry.<br><br>Apache Beam has included a Spark runner since its inception to allow users to execute Beam pipelines on Spark, but until recently the Spark runner could only execute pipelines written in Java. In this talk we will introduce the portability framework and how we adapted it into the existing Spark runner translation to make the Spark runner portable. <br><br>We will show you how to execute Beam pipelines written in Python and Golang in Spark with Beam and invite you to use the new Spark Portable Runner. We will mention the use case of Tensorflow Extended, the end-to-end platform for data validation and transformation and ML model analysis. Finally we will discuss ongoing work and some future plans for the portable runner.",
        "speakers": ["kyle_weaver", "ismal_meja"],
        "title": "Portable Spark Runner: Running Beam Pipelines Written in Python and Go with Spark"
    },
    "1117": {
        "description": "Using Apache Beam to get data in your data lake? In a agile company you don’t want to re-compile your ingestion pipeline every time a sprint finished. In this talk we go over all mechanisms and building blocks you need to make dynamic pipelines really work.<br><br>We’ll see why schemas are so important. How do we get these schemas in our pipelines and discuss methods to protect ourselves from data corruption and incompatible schema evolution. <br><br>The new features like schema aware PCollection get a thorough deep dive and finally we go over real world examples and position Apache Beam in the new PLT (Push Load Transform) world.",
        "speakers": ["alex_van_boxel"],
        "title": "Driving dynamic Beam pipelines"
    },
    "1118": {
        "description": "Hazelcast Jet is a distributed data processing engine that threats all data as a stream. Jet is built on top of Hazelcast IMDG and thanks to this the Jet cluster can also play the role of the data source and sink. If you use Jet this way, you can achieve perfect data locality and top-of-the-class throughputs. <br><br>Jet uses cooperative multithreading (comparable to green threads), it's processors correspond to standalone single-threaded tasks that process streams of data. By not depending on OS-level thread scheduling Jet can achieve greater throughput and better saturate the CPU cores. Since the theory underlying Apache Beam has also influenced the development of Jet and there are many conceptual similarities, attempting to write a Runner based on Jet came naturally to us. When we started however the first big obstacle we hit was the relative lack of documentation. Figuring out what exactly a Runner needed to do required reverse engineering existing runners. A second obstacle turned out to be the generic nature of the user defined functions used by Beam. While Jet's cooperative execution model allows for great performance, it requires the processing of the data to be non-blocking and this is hard to assure when running completely opaque and generic DoFns. Yet another difficulty was presented by trying to adapt Jet's fault tolerance and processing guarantee mechanism for the Runner, a process that, even though difficult, might be more solvable than the above mentioned non-blocking requirement. Despite these difficulties implementing an initial version of our runner turned out to be quite doable, even though experimental it's quite capable of running a was majority of the existing Beam test suits and we are proud to say that we are now listed on the Capability Matrix. We can also showcase Nexmark test results of our runner and describe possible avenues for future development.",
        "speakers": ["jozsef_bartok"],
        "title": "Writing the Hazelcast Jet Runner"
    },
    "1119": {
        "description": "Ride-sharing is a two-sided marketplace; balancing supply and demand with price in real time is critical to maintaining an efficient system. Dynamic pricing creates fairness for drivers (by raising rates when there is a lot of demand) and maintains good experiences for passengers (by satisfying pick-up time SLAs). This complex system makes real-time decisions using various data sources; machine learning models; and a streaming infrastructure for low latency, reliability and scalability. In this streaming infrastructure, our system consumes a massive number of events from different sources to make these pricing decisions.<br><br>Reacting to these events in a cron scheduler based legacy infrastructure with inherent latency becomes a challenge, especially where timely reactions are required to balance market conditions. By leveraging Apache Beam, Lyft’s Streaming Platform powers pricing by bringing together the best of two worlds: ML models in Python and Apache Flink on JVM as the streaming engine.<br><br>Topics covered in this talk include:<br>* A brief discussion of dynamic pricing, including motivation and high-level problem formulation<br>* Comparison of legacy architecture and new streaming architecture<br>* Overview of streaming platform architecture and technology stack<br>* Major gains from streaming architecture<br>* Lessons learned",
        "speakers": ["rakesh_kumar"],
        "title": "Pricing Lyft rides with Apache Beam - a case study in migrating from a worker-based workflow to streaming"
    },    
    "1120": {
        "description": "Access to real-time data is increasingly important for many organizations. At Lyft, we process millions of events per second in real-time to compute prices, balance marketplace dynamics, detect fraud, among many other use cases. To do so, we run dozens of Apache Flink and Apache Beam pipelines. Flink provides a powerful framework that makes it easy for non-experts to write correct, high-scale streaming jobs, while Beam extends that power to our large base of Python programmers.<br><br>Historically, we have run Flink clusters on bare, custom-managed EC2 instances. In order to achieve greater elasticity and reliability, we decided to rebuild our streaming platform on top of Kubernetes. In this session, I'll cover how we designed and built an open-source Kubernetes operator for Flink and Beam, some of the unique challenges of running a complex, stateful application on Kubernetes, and some of the lessons we learned along the way.",
        "speakers": ["micah_wylde"],
        "title": "Running Apache Flink and Apache Beam on Kubernetes"
    },
    "1121": {
        "description": "One of the key components in any data processing system is IO connectors. These fundamental blocks allow to read and write data, which is stored in different type of sources, in a unified and distributed way. In this sense, Apache Beam is not an exception - it provides a rich API to develop a new connector with your favorite SDK and easily integrate it with Beam runners.<br><br>In this talk we are going to show you how to write your own IO connector (in Java). We will see what are the differences between bounded and unbounded sources, how to implement efficient sources and sinks and where we need to pay more attention during the development, what kind of API your connector should provide to users and how it can be tested. To achieve this we will rely on examples from existing Beam connectors.<br><br>We will also give a brief overview of a rather recent feature/pattern in Beam IO - Composable IO connectors and the Splittable DoFn API. We will discuss the advantages of modular IO API design and some new IO design patterns allowed by this style. <br><br>This talk will be interesting for people, who consider to write their own IO connectors or want to contribute to existing ones, as well as for Beam users, who wish to understand how existing Beam connectors work under the hood.",
        "speakers": ["alexey_romanenko", "ismal_meja"],
        "title": "Developing new IO connectors in Apache Beam"
    },
    "1122": {
        "description": "Python is a widely used programming language that is characterized by a low barrier to entry. As many other companies in the industry, Yelp has used Python as the main programming language to implement back-end services. Unfortunately, when it comes to stream processing Python presents several challenges, including performance limitations, lack of proper multi-threading support and limited framework options. <br><br>When the use cases for more advanced stream processing started to arise, at Yelp we decided to leverage Flink and introduce a Scala/Java stack for our Data Pipeline. Over the course of two years we built our connector ecosystem in Flink and exposed to developers a very lightweight stream processing API based on Flink SQL.<br><br>While Flink SQL was quickly adopted by our developers, we soon realized that more complex use cases cannot easily be solved in Flink SQL. At the same time, the alternative engineering cost to develop and maintain a complex production application in an unfamiliar JVM based language was not convenient for our product teams.<br><br>At the beginning of the year, we started looking at Apache Beam with the goal of filling the gap between the high level Flink SQL api and the low level advanced Java Flink streaming API. In this talk I’ll cover the motivations and use cases that brought us to adopt Apache Beam, the challenges we faced integrating Beam with an existing Flink infrastructure and the strategy we followed to deploy and productionize at scale." ,
        "speakers": ["encrico_canzonieri"],
        "title": "Stream processing for the masses with Beam, Python and Flink"
    },
    "1123": {
        "description": "We built a scalable and flexible stream data pipeline for our microservices on Google Cloud Platform (GCP), using Cloud Pub/Sub, Google Cloud Storage, BigQuery, and Cloud Dataflow, using Apache Beam. The stream data pipeline is working on the production system for Mercari, one of the biggest C2C e-commerce services in Japan. The pipeline currently accepts logs from 5+ microservices, and the number will increase soon.<br><br>Our microservice architecture is based on the following three concepts:<br>1. Split the log collection and data processing phases to keep the system simple.<br>2. Use stream processing in order to achieve low latency. <br>3. Don’t just accumulate raw data—support structured output that is easier to use.<br><br>Based on these concepts, we built a data pipeline in GCP, as seen below: https://www.dropbox.com/s/3qx5hq0438aqv35/StreamDataPipelineV2.jpg<br><br>For each microservice, we will provide a Cloud Pub/Sub “Ramp” to send logs to. Cloud Pub/Sub can have messages that contain an optional byte array in their payloads. The entire message that is ingested into the Ramp uses Cloud Dataflow streaming processing to collect them in the “RawDataHub,” a Cloud Pub/Sub topic for collection. When this happens, the PubsubMessage payload is not changed at all; the metadata necessary for subsequent processing (destination and schema information, data necessary for pipeline metrics, etc.) is provided in the PubsubMessage’s attribute map. This Dataflow job does not do processing for each service or topic—it treats all messages uniformly.The raw data from RawDataHub is then output to two independent Cloud DataFlow streaming processes: “RawDataLake”(the infrastructure is in Google Cloud Storage, or “GCS”) and “StructuredDataHub”, another Cloud Pub/Sub topic. The StructuredDataHub has structured avro records. The structured data from StructuredDataHub is then sent to more two independent Cloud DataFlow streaming processes: “StructuredDataLake” on GCS and “Data WareHouse” (Google BigQuery).",
        "speakers": ["shuichi_suzuki"],
        "title": "Creating a Stream Data Pipeline on Google Cloud Platform using Apache Beam"
    },
    "1124": {
        "description": "In this talk we are going to cover how we have leveraged portability of Beam and make Stream Processing in Python possible on top of Apache Samza. We will first touch points on Apache Samza in general, and how it stands out as the stream processing engine at LinkedIn that scales to over a trillion messages processed per day, with strong state support and flexible deployment models. Next we introduce Samza Runner for Beam, particularly the portable runner. We will cover the efforts we have done to leverage Beam and make stream processing in Python available at LinkedIn, followed by a few use cases. The talk will conclude with our plan for future work.",
        "speakers": ["hai_lu"],
        "title": "Samza Portable Runner for Beam"
    },
    "1125": {
        "description": "Learning a streaming framework like Apache Beam is exciting but with Hello World! examples aren't fun. There aren't a lot of free and interesting streaming data sources for beginners to play with, it's very easy to give up learning if it's boring. To keep myself learning something new, I find myself need incentives and accomplishments to continue. If you are like me need some motivation to keep learning, this talk will give you some inspiration.<br><br>In this talk, I will share my experience of learning Apache Beam by showing demos to you on how I create a streaming data with my custom 'Marvel Fights Stream Producer'. I will discuss how I went through with the Apache Beam Programming Guide and replaced the official examples with Marvel streaming data I produced.  I will also talk about what I learn from creating my custom data stream producer and how that helps me learn Apache Beam better.",
        "speakers": ["chengzhi_zhao"],
        "title": "Creating Custom Streaming Events to Learn Apache Beam"
    },
    "1126": {
        "description": "In this talk we will discuss the infrastructure  Apache Beam has built to create a positive contributor experience. Over the last few years we have developed tooling, refactored code, instrumented Jenkins builds, collected metrics, created a metrics dashboard, and involved the community in order to ensure happy contributors. We will discuss the importance of engineering productivity and tooling in developing strong communities, where we have succeeded, and where we still have work to do.",
        "speakers": ["jason_kuster", "alan_myrvold"],
        "title": "Friendlier Communities Through Infrastructure: Apache Beam’s Journey"
    },
    "1127": {
        "description": "We’ve started to provide our stream based data pipeline by using Google Cloud Dataflow and Apache Beam since 2018 fall. It collects event logs from microservices running on GKE, then transforms and forwards the logs to GCS and BigQuery to use for analytics, etc. As you know, implementing and operating streaming jobs are challenging. We’re encountered various issues during that time.<br><br>I’d like to share our knowledge on development and operation perspective. There are 3 topics in the development part, 1) Implementing stream jobs with using spotify/scio, 2) How to debug the jobs, especially having DynamicDestination, 3) How to load testing, to ensure our jobs stable. And topics in the next part, 1) How to deploy new jobs safely(with avoiding data loss), 2) How to monitor the jobs and surrounding systems, and misc.",
        "speakers": ["ryo_okubo"],
        "title": "Production-ready stream data pipeline in Merpay, Inc"
    },
    "1128": {
        "description": "Ludwig is a code-free Deep Learning toolbox based on TensorFlow open-sourced by Uber AI Labs. Ludwig is unique in its ability to help make deep learning easier to understand for non-experts and enable faster model improvement iteration cycles for experienced machine learning developers and researchers alike. By using Ludwig, experts and researchers can simplify the prototyping process and streamline data processing so that they can focus on developing deep learning architectures rather than data wrangling.<br><br>Ludwig introduces the notion of data type-specific encoders and decoders, which results in a highly modularized and extensible architecture: each type of data supported (text, images, categories, and so on) has a specific preprocessing function. <br><br>In this talk, we’ll be looking at building Beam pipelines to programmatically create Deep Learning models with Ludwig for different input data types for both model training and inference using Beam-Python SDK. We will be showing 2 examples of training deep learning classifiers with text and images on an unbounded source and running inference on that.",
        "speakers": ["suneel_marthi"],
        "title": "Beaming Deep Learning with Ludwig"
    },
    "1129": {
        "description": "Apache Beam provides a unified programming model to execute batch and streaming pipelines on all the popular big data engines. The translation layer from Beam to the chosen big data engine is called a runner.<br><br>The current runner for Apache Spark is based on the RDD/DStream framework. However, there is an ongoing work to move it to Spark next generation framework a.k.a structured streaming. This talk will present why structured streaming is a good fit for Apache Beam, why it is worth the effort, and will give some feedback on how Apache Beam has solved the challenge, what the tough points and the sweet points were.",
        "speakers": ["etienne_chauchot"],
        "title": "The journey of building a Beam runner based on Spark structured streaming framework"
    },
    "1130": {
        "description": "Apache Beam provides a unified programming model to execute batch and streaming pipelines on all the popular big data engines. The translation layer from Beam to the chosen big data engine is called a runner.<br><br>The current runner for Apache Spark is based on the RDD/DStream framework. However, there is an ongoing work to move it to Spark next generation framework a.k.a structured streaming. This talk will present why structured streaming is a good fit for Apache Beam, why it is worth the effort, and will give some feedback on how Apache Beam has solved the challenge, what the tough points and the sweet points were.",
        "speakers": ["khai_tran"],
        "title": "Unifying Batch and Stream Data Processing with Apache Calcite and Apache Beam"
    },
    "1131": {
        "description": "This workshop will explore the basic concepts of the Beam model and the Beam API especially in Python and Java.  There will be lectures on Beam and the Ecosystem, hands-on exercises in basic beam usage, we will dig into common usage patterns and end with tips and exercises for running Beam in Production.",
        "speakers": ["austin_bennett", "pablo_estrada"],
        "title": "Beam Introductory Workshop"
    },
    "1132": {
        "description": "Apache Beam's early vision was to create a framework agnostic of execution engines or languages. This vision was not fully realized from the early beginnings. At first, Beam was only agnostic of executions engines but very much tight to one language: Java. <br><br>In the recent years much effort has been undergone to implement a language portability layer, to finally realize the vision of portability.<br><br>In this talk we want to take a look at the state of portability, how it works, and what new use cases it unlocks.",
        "speakers": ["max_michels", "thomas_weise"],
        "title": "The Road To Portability"
    },
    "1133": {
        "description": "Developing ML and deep learning applications to be deployed in production is much more than just training a model. Google has taken years of experience in developing production ML pipelines and offered the open source community TensorFlow Extended (TFX), an open source version of the ML platform that Google uses internally. Pipeline processing is a core requirement of any production ML platform, and the TFX has chosen Apache Beam to implement their pipeline. <br><br> Learn from Google’s experience in applying Beam for ML pipelines, including how TFX uses Beam and why Beam was chosen.",
        "speakers": ["charles_chen"],
        "title": "Apache Beam for Production Machine Learning: TensorFlow Extended (TFX)"
    }   
}
