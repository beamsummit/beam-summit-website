{
    "1": {
        "icon": "registration",
        "title": "Registration & morning coffee",
        "image": "https://firebasestorage.googleapis.com/v0/b/hoverboard-experimental.appspot.com/o/images%2Fbackgrounds%2Fregistration.jpg?alt=media&token=27328646-d323-4cca-904c-75f021bc3ffe",
        "description": "Get your badge, coffee, enjoy talking to each other"
    },
    "2": {
        "description": "Foooooooood!",
        "icon": "lunch",
        "title": "Lunch",
        "image": "https://firebasestorage.googleapis.com/v0/b/hoverboard-experimental.appspot.com/o/images%2Fbackgrounds%2Flunch.jpg?alt=media&token=bc82aaff-92cc-4dcc-a00e-2b6f30a40225"
    },
    "3": {
        "description": "Coffeeeeeee!",
        "icon": "coffee-break",
        "title": "Coffee break",
        "image": "https://firebasestorage.googleapis.com/v0/b/hoverboard-experimental.appspot.com/o/images%2Fbackgrounds%2Fcoffee-break.jpg?alt=media&token=7c2c929b-cb94-4be8-a05a-b3f678136cb7"
    },
    "4": {
        "description": "Coffeeeeeee!",
        "icon": "coffee-break",
        "title": "Morning coffee",
        "image": "https://firebasestorage.googleapis.com/v0/b/hoverboard-experimental.appspot.com/o/images%2Fbackgrounds%2Fmorning.jpg?alt=media&token=d0da3d64-fc63-48c6-9d41-1460451dca64"
    },
    "5": {
        "description": "Official start of the conference. Greetings from the organizers, sponsors and partners.",
        "icon": "opening",
        "title": "Beam Summit Europe 2019 Opening Ceremony",
        "image": "https://firebasestorage.googleapis.com/v0/b/hoverboard-experimental.appspot.com/o/images%2Fbackgrounds%2Fopening.jpg?alt=media&token=aa52dd45-50c9-4e36-9485-c1f9138ecd6b",
        "language": "English"
    },
    "6": {
        "description": "Afterparty is a great time to have some fun and meet new people",
        "icon": "party",
        "title": "Afterparty & Networking",
        "image": "https://firebasestorage.googleapis.com/v0/b/hoverboard-experimental.appspot.com/o/images%2Fbackgrounds%2Fparty.jpg?alt=media&token=8e413f01-373a-40bf-bb87-1dcf1d8e6af1"
    },
    "7": {
        "description": "Make sure you won't miss closing ceremony!",
        "title": "Conference closing.",
        "image": "https://firebasestorage.googleapis.com/v0/b/hoverboard-experimental.appspot.com/o/images%2Fbackgrounds%2Fclosing.jpg?alt=media&token=f02d014a-09ee-407e-88c7-b74093e00fed"
    },
    "8": {
        "title": "Keynote",
        "language": "English",
        "description": ""
    },
    "9": {
        "title": "TBD",
        "language": "English",
        "description": "TBD"
    },
    "100": {
        "presentation": "https://speakerdeck.com/gdglviv/mete-atamel-windows-and-net-on-google-cloud-platform",
        "complexity": "Beginner",
        "tags": [
            "Use case talk"
        ],
        "speakers": [
            "matthias_baetens"
        ],
        "title": "",
        "language": "English",
        "description": ""
    },
    "101": {
        "videoId": "fLfQNp75kbc",
        "language": "English",
        "description": "",
        "presentation": "https://speakerdeck.com/gdglviv/sascha-wolter-the-physical-web-context-is-king",
        "complexity": "Beginner",
        "tags": [
            "Technical talk"
        ],
        "speakers": [
            "matthias_baetens"
        ],
        "title": ""
    },
    "1000": {
        "title": "Keynotes",
        "description": "Welcome and Community Updates. <br> The Road to Portability. <br> Recent developments in the Apache Beam Python SDK. <br> Making Beam Simpler and More Powerful.",
        "speakers": ["matthias_baetens", "joana_carrasqueira", "griselda_cuevas", "maximilian_michels", "thomas_weise", "robert_bradshaw", "robbe_sneyders", "reuven_lax"]
    },
    "1001": {
        "description": null,
        "tags": [],
        "complexity": [],
        "speakers": [],
        "title": "Keynote"
    },
    "1002": {
        "speakers": [],
        "title": "Coffee Break",
        "description": null,
        "tags": [],
        "complexity": []
    },
    "1003": {
        "description": null,
        "tags": [],
        "complexity": [],
        "speakers": [],
        "title": "Coffee Break"
    },
    "1004": {
        "title": "Workshop Morning",
        "description": "The workshop will be a hands-on overview to Beam.  We will start with a lecture on Apache Beam and Beam model. Hands on examples and exercises will be provided to build up understanding of the basic Beam operations in different languages, and later working our way up to running on various runners, and more complex and advanced use-cases.  At the end of the day attendees with understand the vision for portability and the Beam model and have gained experienced writing and running Beam pipelines.<br><br> 11:00 - 12:00    Presentation on Beam SDK Tutorial. <br> 12:00 - 12:30    Loading and start working on the Beam Katas.",
        "speakers": ["pablo_estrada"]
    },
    "1005": {
        "description": null,
        "tags": [],
        "complexity": [],
        "speakers": [],
        "title": "Lunch"
    },
    "1006": {
        "description": null,
        "tags": [],
        "complexity": [],
        "speakers": [],
        "title": "Lunch"
    },
    "1007": {
        "title": "Workshop Afternoon",
        "description": "The workshop will be a hands-on overview to Beam.  We will start with a lecture on Apache Beam and Beam model. Hands on examples and exercises will be provided to build up understanding of the basic Beam operations in different languages, and later working our way up to running on various runners, and more complex and advanced use-cases.  At the end of the day attendees with understand the vision for portability and the Beam model and have gained experienced writing and running Beam pipelines. <br><br> 12:30 - 13:30    Lunch (and work on the Katas with instructors). <br> 13:30 - 14:30    Further work on Katas. <br> 14:30 - 15:30    Presentation on advanced Beam concepts (SDF/State/Timers). <br> 15:30 - 17:00    Submitting Beam jobs to Flink clusters / Google Cloud Dataflow.",
        "speakers": ["pablo_estrada"]
    },
    "1008": {
        "description": null,
        "tags": [],
        "complexity": [],
        "speakers": [],
        "title": "Coffee Break"
    },
    "1009": {
        "description": null,
        "tags": [],
        "complexity": [],
        "speakers": [],
        "title": "Coffee Break"
    },
    "1010": {
        "speakers": [],
        "title": "Coffee Break",
        "description": null,
        "tags": [],
        "complexity": []
    },
    "1011": {
        "description": null,
        "tags": [],
        "complexity": [],
        "speakers": [],
        "title": "Lunch"
    },
    "1012": {
        "speakers": [],
        "title": "Lunch",
        "description": null,
        "tags": [],
        "complexity": []
    },
    "1013": {
        "speakers": [],
        "title": "Coffee Break",
        "description": null,
        "tags": [],
        "complexity": []
    },
    "1015": {
        "title": "Contributing to Beam Workshop",
        "description": "The workshop will cover contributing to Beam. We will start with a lecture on typical journeys through the open source contributor's lifecycle. Then the attendee will be shown the workflows and tools, including general procedures for how individuals are encouraged to operate within the community. For those inclined, the goal will be to get people setup all the way through making a real (code/documentation) contribution by submitting a pull request.",
        "speakers": ["austin_bennett"]
    },
    "102915": {
        "description": "Replacing the entry point for IoT data into a system already ingesting and processing data from thousands of devices worldwide can seem like a terrifying and risky endeavor.\r\nIn this talk I'll share our experience at Augury of doing just that - migrating a traditional architecture, based on APIs and services, to a streaming solution implemented using Beam. I'll demonstrate how we were able to leverage advanced features of the Beam model, such as stateful transforms and timers, to preserve legacy behavior and effectively bake backwards-compatibility into our ingestion pipeline. \r\nFinally, I'll discuss how taking this approach has enabled us to incrementally and separately evolve downstream processing components without losing existing capabilities.",
        "tags": [
            "Talk",
            "Use-Case"
        ],
        "complexity": "Intermediate",
        "speakers": [
            "amit_ziv-_kenet"
        ],
        "title": "How we migrated our IoT data ingestion pipeline to Beam without missing a bit"
    },
    "102954": {
        "description": "There is a number of new Systems that support running SQL queries on top of data streams. Apache Beam has not been the exception. It now provides a SQL transform that allows anyone to analyze streams from any source interactively and it supports running the same query on multiple runners including Flink, Spark, and Google Cloud Dataflow. We are also working to support mixed-language operations.\r\n\r\nThis talk starts by showing how the Beam SQL feature works, demoing a couple simple use cases: a pure SQL pipeline as well as SQL embedded in a Java pipeline. Then we review how streaming SQL came from collaboration between the Beam, Calcite and Flink communities and the advantages over other SQL implementations. Finally, we will deep-dive into the architecture of Beam’s implementation, as well as the design decisions that were taken along the way to build the feature, and how they have turned out.",
        "tags": [
            "Technical",
            "Talk"
        ],
        "complexity": "Intermediate",
        "speakers": [
            "pablo_estrada",
            "andrew_pilloud"
        ],
        "title": "Building an interactive streaming SQL engine in Beam"
    },
    "103808": {
        "speakers": [
            "reza_ardeshir_rokni",
            "tyler_akidau"
        ],
        "title": "Solving for Timerseries with Apache Beam",
        "description": "Time series processing requires ordered processing of data points, often across windows boundaries. Tricky stuff for distributed processing systems, in this chat we look at how to solve for these problems using a combination of Global Windows, State and the Timer API's. The session will also expand on a new concept called Validity Windows.",
        "tags": [
            "Use-Case",
            "Talk"
        ],
        "complexity": "Advanced"
    },
    "104213": {
        "description": "Cloud Dataflow is Google's data processing service that uses Apache Beam as its SDK. Sergei Sokolenko, the product manager for Cloud Dataflow, will go into the details of operating the Dataflow service and lead a discussion around future work in Beam and Dataflow.   ",
        "tags": [
            "Technical",
            "Talk"
        ],
        "complexity": "Intermediate",
        "speakers": [
            "sergei_sokolenko"
        ],
        "title": "Beam-on-Dataflow deep dive and roadmap"
    },
    "114542": {
        "description": "Kettle is an open source data integration tool under an Apache license. This session will demonstrate how we integrated with Apache Beam to allow you to visually build pipelines that can be executed on the various Beam runners without the need to write any code.  \r\nAfter an introduction on the Kettle project you will get an overview of the supported functionalities and best practices with a number of demos on DataFlow, Spark and Flink. ",
        "tags": [
            "Technical",
            "Talk"
        ],
        "complexity": "Introductory and overview",
        "speakers": [
            "matt_casters"
        ],
        "title": "Visual Beam Pipeline Development with Kettle"
    },
    "114625": {
        "description": "This talk is reflecting on the transformation that happened in the past year since I joined Ricardo.ch's Data Intelligence team. We went from a on-premise data warehouse fed by batch-loaded dumps to a BigQuery based solution that is using Apache Beam jobs running on an Apache Flink cluster. This cluster was born on-premise and then moved to the cloud when our datacenter was finally shut down in March this year.",
        "tags": [
            "Talk",
            "Use-Case"
        ],
        "complexity": "Introductory and overview",
        "speakers": [
            "tobias_kaymak"
        ],
        "title": "From database dumps to streaming - Ricardo.ch's Beam journey"
    },
    "114962": {
        "tags": [
            "Technical",
            "Talk"
        ],
        "complexity": "Intermediate",
        "speakers": [
            "won_wook_song"
        ],
        "title": "Running Apache Beam programs on Apache Nemo",
        "description": "This talk introduces Apache Nemo, a data processing system that enables flexible optimizations for various execution scenarios, and shares the experiences on building the Apache Nemo Runner for Apache Beam applications (which is now officially supported!). We discuss about the pros and cons of using Apache Beam applications on our system, and demonstrate what we can do with Apache Beam on Apache Nemo with a demo."
    },
    "116266": {
        "speakers": [
            "robert_burke"
        ],
        "title": "A Guided Walkthrough of the Go SDK",
        "description": "This session will walk through the internals of what the Go SDK does to execute a pipeline. Describing the structure of the SDK, where code is located, and the why of the structure, and how it does it with the particular features of the Go language, which notably has strict types but no generics.\r\n\r\nParticular effort will be spent on what the Go SDK does for ease of use, and to reduce DoFn execution overhead.",
        "tags": [
            "Technical",
            "Talk"
        ],
        "complexity": "Intermediate"
    },
    "116289": {
        "speakers": [
            "robert_crowe", "gus_katsiapis"
        ],
        "title": "Apache Beam for Production Machine Learning: TensorFlow Extended (TFX)",
        "description": "Developing ML and deep learning applications to be deployed in production is much more than just training a model.  Google has taken years of experience in developing production ML pipelines and offered the open source community TensorFlow Extended (TFX), an open source version of the ML platform that Google uses internally.  Pipeline processing is a core requirement of any production ML platform, and the TFX has chosen Apache Beam to implement their pipeline.\r\n\r\nLearn from Google’s experience in applying Beam for ML pipelines, including how TFX uses Beam and why Beam was chosen.  Robert Crowe is a member of the TFX team at Google and will discuss the issues and lessons learned of developing and deploying ML and deep learning applications in production using Beam and TFX.",
        "tags": [
            "Technical",
            "Talk"
        ],
        "complexity": "Intermediate"
    },
    "116424": {
        "description": "This use case is about how we used apache beam to analyze and process, in near-real time, football game stream feeds. The goal was to determine events (start of game, team detection, player tracking, ball tracking) and performing analytics on these videos (duration, ball possession, score, ..). The client for which we implemented this use case is https://www.sporttotal.tv/.\r\n\r\nIn particular, Apache Beam Dataflow runner was used with Python SDK to create streaming pipelines. In this pipeline we used sliding windows to chunk the video frames into sets of sequence of frames, which could be used as input for different machine learning models.  Our machine learning models are hosted on GPUs via TF-serving on Kubernetes. The output of the machine learning models were written to Google Cloud Bigtable, which was used by a front-end player to visualize these features on top of the stream feed (with a small delay).\r\n\r\nOther components used: Google Cloud Pub/Sub, Google Kubernetes Engine, Google Cloud Bigtable",
        "tags": [
            "Talk",
            "Use-Case"
        ],
        "complexity": "Intermediate",
        "speakers": [
            "sven_degroote"
        ],
        "title": "Video Analytics for Football games @ SportTotal.tv"
    },
    "117239": {
        "description": "One of the key components in any data processing system is IO connectors. These fundamental blocks allow to read and write data, which is stored in different type of sources, in a unified and distributed way. In this sense, Apache Beam is not an exception - it provides a rich API to develop a new connector with your favorite SDK and easily integrates it with Beam runners.\r\n\r\nIn this talk I'm going to show you how to write your own IO connector (in Java). We will see what are the differences between bounded and unbounded sources, how to implement efficient sources and sinks and where we need to pay more attention during the development, what kind of API your connector should provide to users and how it can be tested. To achieve this we will rely on examples from existing Beam connectors.\r\n\r\nThis talk will be interesting for people, who consider to write their own IO connectors or want to contribute to existing ones, as well as for Beam users, who wish to understand how existing Beam connectors work under the hood.\r\n",
        "tags": [
            "Talk",
            "Technical"
        ],
        "complexity": "Intermediate",
        "speakers": [
            "alexey_romanenko"
        ],
        "title": "Developing new IO connectors in Apache Beam"
    },
    "117290": {
        "tags": [
            "Technical",
            "Talk"
        ],
        "complexity": "Intermediate",
        "speakers": [
            "fabian_hueske",
            "tyler_akidau"
        ],
        "title": "One SQL to Rule Them All – a Syntactically Idiomatic Approach to Management of Streams and Tables",
        "description": "Apache Calcite is a data management framework that includes a SQL parser and query optimizer. It is used by many projects that implement SQL processing capabilities, including Apache Beam and Apache Flink. Over the last years, members of these three communities had many discussions about the semantics and syntax of \"Streaming SQL\". End of last year, we decided to formalize and summarize our views and ideas in paper that we submitted to the Industrial Track of the SIGMOD 2019 conference. The paper got accepted (http://sigmod2019.org/sigmod_industry_list).\r\n\r\nIt presents a three-part proposal for integrating robust streaming into SQL, namely: \r\n(1) time-varying relations as a foundation for classical tables as well as streaming data,\r\n(2) event time semantics, \r\n(3) a limited set of optional keyword extensions to control the materialization of time-varying query results.\r\n\r\nThe paper shows how with these minimal additions it is possible to utilize the complete suite of standard SQL semantics to perform robust stream processing and motivates and illustrate these concepts using examples and describe lessons learned from implementations in Apache Calcite, Apache Flink, and Apache Beam. \r\n\r\nIn this talk, we present our \"Syntactically Idiomatic Approach to Manage Streams and Tables\"."
    },
    "117313": {
        "tags": [
            "Technical",
            "Talk"
        ],
        "complexity": "Intermediate",
        "speakers": [
            "steffen_hausmann"
        ],
        "title": "Unify Batch and Stream Processing with Apache Beam on AWS",
        "description": "One of the big visions of Apache Beam is to provide a single programming model for both batch and streaming that runs on multiple execution engines.\r\n\r\nIn this session, we explore an end to end example that shows how you can combine batch and streaming aspects in one uniform Beam pipeline: We start with ingesting taxi trip events into an Amazon Kinesis data stream and use a Beam pipeline to analyze the streaming data in near real time. We then show how to archive the trip data to Amazon S3 and how we can extend and update the Beam pipeline to generate additional metrics from the streaming data moving forward. We subsequently explain how to backfill the added metrics by executing the same Beam pipeline in a batch fashion against the archived data in S3. Along the way we furthermore discuss how to leverage different execution engines, such as, Amazon Kinesis Data Analytics for Java and Amazon Elastic Map Reduce, to run Beam pipelines in a fully managed environment.\r\n\r\nSo you will not only learn how you can leverage Beam's expressive programming model to unify batch and streaming you will also learn how AWS can help you to effectively build and operate Beam based streaming architectures with low operational overhead."
    },
    "117419": {
        "speakers": [
            "alex_van_boxel"
        ],
        "title": "Protobeam - Protobuf power in the Beam pipeline",
        "description": "Although beam has native Protobuf support, still we created the *protobeam* project too augment the typed message format to integrate with several backends (like Bigtable, BigQuery, Spanner, ...). The talk will also show how you can use annotations too change the behaviour of a message.\r\n\r\nThis talk will show all the integration point within Apache Beam to make the usage transparent and idiomatic (integration with the registry, Beam SQL, ...).\r\n\r\nAs the project is in active development, but already usable, we'll discuss the roadmap and the direction *protobeam* is heading to make dynamic pipelines based on Protobuf a pleasure to use.",
        "tags": [
            "Talk",
            "Technical"
        ],
        "complexity": "Intermediate"
    },
    "117434": {
        "description": "In this talk, we present our use case for periodically ingesting legacy data from SQL server into Google Bigquery using Apache Beam. As for us the ability to handle schema changes is a hard requirement, legacy table schemas are turned into data contracts. These contracts are used in our batch pipelines, allowing us to generate BigQuery table schemas and to convert our data accordingly.\r\n\r\nThe talk goes into detail of how we used our ProtoBeam project to achieve this. Next to this, it presents features for tweaking bigquery tables based on contracts, like column renaming and time partitioning, to make analysts’ life easier.\r\n",
        "tags": [
            "Talk",
            "Use-Case"
        ],
        "complexity": "Introductory and overview",
        "speakers": [
            "wout_scheepers"
        ],
        "title": "FrankenBatch: ingesting legacy data into BigQuery using data contracts"
    },
    "117436": {
        "description": "Ludwig is a code-free Deep Learning toolbox based on TensorFlow open-sourced by Uber AI Labs.  Ludwig is unique in its ability to help make deep learning easier to understand for non-experts and enable faster model improvement iteration cycles for experienced machine learning developers and researchers alike. By using Ludwig, experts and researchers can simplify the prototyping process and streamline data processing so that they can focus on developing deep learning architectures rather than data wrangling.\r\n\r\nLudwig introduces the notion of data type-specific encoders and decoders, which results in a highly modularized and extensible architecture: each type of data supported (text, images, categories, and so on) has a specific preprocessing function. \r\n\r\nIn this talk, we’ll be looking at building Beam pipelines to programmatically create Deep Learning models with Ludwig for different input data types for both model training and inference using Beam-Python SDK. We will be showing 2 examples of training deep learning classifiers with text and images on an unbounded source and running inference on that.",
        "tags": [
            "Talk",
            "Technical"
        ],
        "complexity": "Advanced",
        "speakers": [
            "suneel_marthi"
        ],
        "title": "Beaming Deep Learning with Ludwig"
    },
    "117693": {
        "description": "Apache Beam doesn’t have any knowledge of the actual structure of the records in a PCollection, and little understanding of PTransforms. In practice, most of the PCollections are schematized: Avro records, BigQuery rows, and even POJOs and case classes. Many operations are performed on structural records: filtering by field, grouping by a specific field, and so on.\r\n\r\nIn this talk, we are going to learn about schema-aware PCollections and Beam SQL. See how we can leverage them, and how it works with Scio, Scala DSL for Apache Beam.",
        "tags": [
            "Technical",
            "Talk"
        ],
        "complexity": "Intermediate",
        "speakers": [
            "gleb_kanterov"
        ],
        "title": "Schema-aware PCollections and Beam SQL"
    },
    "117726": {
        "description": "Apache Beam is a key technology for building scalable End-to-End ML pipelines, as it is the data preparation and model analysis engine for TensorFlow Extended (TFX), a framework for horizontally scalable Machine Learning (ML) pipelines based on TensorFlow. In this talk, we present TFX on Hopsworks, a fully open-source platform for running TFX pipelines on any cloud or on-premise. Hopsworks is a project-based multi-tenant platform for both data parallel programming and horizontally scalable machine learning pipelines. Hopsworks supports Apache Flink as a runner for Beam jobs and TFX pipelines are supported through Airflow support in Hopsworks. We will demonstrate how to build a ML pipeline with TFX, Beam’s Python API and the Flink Runner by using Jupyter notebooks, explain how security is transparently enabled with short-lived TLS certificates, and go through all the pipeline steps, from Data Validation, to Transformation, Model training with TensorFlow, Model Analysis, Model Serving and Monitoring with Kubernetes.  \r\nTo the best of our knowledge, Hopsworks is the first fully open-source on-premise platform that supports both TFX  pipelines and Apache Beam. ",
        "tags": [
            "Technical",
            "Talk"
        ],
        "complexity": "Intermediate",
        "speakers": [
            "jim_dowling",
            "ahmad_al-_shishtawy",
            "theofilos_kakantousis"
        ],
        "title": "End-to-End ML pipelines with Beam, Flink, TensorFlow, and Hopsworks"
    },
    "117836": {
        "speakers": [
            "ukasz_gajowy",
            "micha_walenia",
            "katarzyna_kucharczyk"
        ],
        "title": "Quality Assurance in Beam: measure your pipeline!",
        "description": "Apache Beam being a Unified Model supports multiple Runners and SDKs. You also want it to work like a charm on multiple file systems and be sure that any IO is fine with big loads. Other than that you need your jobs to be efficient (and you probably want to know “all the numbers”). How to ensure that all your goals are met? You need proper testing for that! However, due to the complex nature of the problem itself, automated testing can sometimes be tricky too!\r\n\r\nLuckily, we’ve provided a whole bunch of tools to Beam's codebase to ease your pain. In this presentation, we will show you how you can write various integration/performance tests in Beam using an auto-setup infrastructure (DBs, filesystems & runners), BigQuery and Metrics API. We will also show you how do we currently do it in Beam in both IO Integration Tests or Load Tests of core Beam transforms. \r\n\r\nUse these tools to increase the quality of your beam pipelines!\r\n",
        "tags": [
            "Technical",
            "Talk"
        ],
        "complexity": "Introductory and overview"
    },
    "117858": {
        "speakers": [
            "ryan_skraba"
        ],
        "title": "Using Beam IOs for data discovery and exploration",
        "description": "The primary goal of a Beam IO is to ingest distributed data to the cluster.  We've identified another interesting use case -- using the same Beam IOs for data discovery and interactive exploration of a Beam pipeline, much like what you would see in a Beam notebook.  In this talk, we discuss the challenges, some of the necessary workarounds and best practices for using existing Beam IOs to quickly investigate the data that will eventually be found in the full pipeline, including performance, security, schema-on-read and sampling.  We’ll show a short demo of integrating a Beam IO, and exploiting it immediately in Talend Pipeline Designer, a user-friendly front-end of Apache Beam in the Cloud.\r\n",
        "tags": [
            "Talk",
            "Technical"
        ],
        "complexity": "Intermediate"
    },
    "117898": {
        "speakers": [
            "ismal_meja",
            "david_moravek"
        ],
        "title": "Spark Runner (R)evolution",
        "description": "Apache Spark is one of the most popular analytics engines for large-scale data processing, it supports multiple cluster types, e.g. Hadoop, Mesos and Kubernetes. For these reasons it is an important target for Apache Beam. Existing Spark users can take advantage of Beam's rich unified model and its nice APIs without requiring big infrastructure changes or sacrificing their operational knowledge.\r\n\r\nIn this talk we present the history of the Spark runner in Apache Beam, we will discuss in detail how the runner works, we will focus in its current implementation and we will discuss some improvements and optimizations done in recent months.\r\n\r\nThis year has seen a renewed interest in the Spark runner due to two lines of work:\r\n1. A new translation based on Spark's Structured Streaming API which allows a unification of Batch and Streaming in the runner code and let users benefit of some Spark engine optimizations and the new continuous processing mode.\r\n2. Support for different languages (e.g. python) on the Spark runner by translating pipelines using Beam's new Portability APIs.\r\nIn this talk we will also present both efforts and motivate assistants to contribute to the ongoing Spark runner (r)evolution.\r\n",
        "tags": [
            "Technical",
            "Talk"
        ],
        "complexity": "Advanced"
    },
    "118103": {
        "speakers": [
            "xinyu_liu"
        ],
        "title": "Streaming Pipelines at Scale with Apache Beam and Samza",
        "description": "In this talk we will share details of the Samza Beam Runner, which in about a year, has evolved from an initial prototype to a fully fledged Beam runner integrated with the Beam portability framework. The talk will do a deep dive on Samza features that make it one of the most scalable open-source Beam runners in the industry that can process millions of records a day in production. The talk will also discuss the use cases powered by Beam Samza Runner at LinkedIn, including machine learning applications, security applications, and programmatically generated pipelines from offline batch scripts (Pig/Hive). Last but not least, we’ll present our future road map, including multi-language support (Python & Go), and the new Samza features that will benefit both open source projects.",
        "tags": [
            "Technical",
            "Talk"
        ],
        "complexity": "Intermediate"
    },
    "119005": {
        "description": "Uses of Machine Learning are pervasive in today’s world. From recommendations systems to ads serving. In the world of ride sharing we use Machine Learning to make a lot of predictions in realtime, for example: supply/demand curves are used to get an accurate ETA(estimated time of arrival) and fair pricing. Patterns of user behaviour is used to detect fraudulent activities on the platform. This ensures that we provide a good experience to both our drivers and passengers. Insights drawn from raw data becomes less useful over time, so it is important to generate features in near real time to aid in decision making with the most recent view of the world.\r\n\r\nIn this session, we will be talking about how we build a declarative, fully managed, self service Machine Learning platform at Lyft using Apache Flink and Beam and the lessons learned along the way.",
        "tags": [
            "Use-Case",
            "Talk"
        ],
        "complexity": "Advanced",
        "speakers": [
            "sherin_thomas"
        ],
        "title": "Machine Learning with Flink and Beam"
    },
    "119054": {
        "tags": [
            "Technical",
            "Talk"
        ],
        "complexity": "Intermediate",
        "speakers": [
            "aizhamal_nurmamat_kyzy",
            "riona_mac_namara"
        ],
        "title": "Apache Beam Needs You: Getting started with open source contributions",
        "description": "Open source draws its power and strength from the communities that build it. Diversity of perspective and accountability make projects stronger and healthier, and build a more robust ecosystem. We understand that getting started contributing to any project can be scary. But as a new contributor, you have a superpower: You see the project with clear eyes, and you can understand and empathize with - far better than experienced contributors - the challenges faced by new users. In this talk, we'll cover exactly why your fresh perspectives are needed and valued, and share research that demonstrates the very real impact your contributions can bring to the Apache Beam project and its community."
    },
    "119235": {
        "tags": [
            "Talk",
            "Use-Case"
        ],
        "complexity": "Introductory and overview",
        "speakers": [
            "christiaan_swart"
        ],
        "title": "Large scale Natural Language Processing of biomedical literature in Python with beam and spacy",
        "description": "We use beam to extract the relations between entities such as genes, drugs, and diseases from biomedical literature and build a knowledge graph from the extracted relations. Using the knowledge graph to match existing drugs to rare diseases, Healx is on a mission to advance 100 rare disease treatments towards the clinic by 2025. \r\n\r\nBeam allows us to build a knowledge graph encapsulating these relations at scale. We can process about 30 million PubMed abstracts to build our internal knowledge graph in less than 30 hours. Using Dataflow to run our beam job allows us to quickly scale a large cluster up and down depending on the computational needs. The potential for streaming in documents means we don’t need to rebuild our knowledge graph and can continuously push updates from novel publications. Developing and running beam jobs in Python still has some challenges which I will also talk about.\r\n"
    },
    "117839": {
        "speakers": [
            "ukasz_gajowy"
        ],
        "title": "Metrics API - how easy it is!",
        "description": "There are cases when you want to dig deeper and get to know what’s going on in your data processing pipeline. This is what Metrics API is for. In this short flash talk, I’m going to show you how easily you can collect relevant metrics from your data processing pipelines to get to know them better. I’ll also show you some examples from IO, Load and Nexmark tests, where we use this API on a daily basis.\r\n",
        "tags": [
            "Lightning Talk",
            "Technical"
        ],
        "complexity": "Introductory and overview"
      },
      "117493": {
          "speakers": [
              "kannappan_sirchabesan"
          ],
          "title": "Side Input Metrics in Dataflow",
          "description": "Performance is an important consideration in any Data Processing job. The ability to capture metrics in each of the steps within the Beam pipeline job becomes paramount in identifying and resolving performance issues. Cloud Dataflow provides the capability to measure the metrics of Side Inputs used in Beam pipelines. \r\n\r\nSide Input metrics in Dataflow show how the side input access patterns and algorithms affect a pipeline's performance. Dataflow writes Side input collections to a persistent layer, such as a disk, and Beam Transforms read from this persistent collection. These reads and writes affect a job's execution time. Time spent writing, the number of bytes written, time spent reading by each consumer, the number of bytes read by each consumer, etc., are some of the common metrics captured and displayed by Cloud Dataflow. \r\n\r\nTypical performance issues are observed when using large Side Inputs as part of a join operation, or in the process of repetitive fetching of side inputs as part of Reiteration.\r\n",
          "tags": [
              "Lightning Talk",
              "Technical"
          ],
          "complexity": "Introductory and overview"
      }
}
